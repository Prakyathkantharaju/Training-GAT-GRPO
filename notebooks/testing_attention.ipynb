{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prakyath/developments/training/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using offset mappings (most efficient)\n",
    "def find_token_positions_with_offsets(tokenizer, text, search_strings):\n",
    "    \"\"\"\n",
    "    Find token positions for search strings using offset mappings\n",
    "    \"\"\"\n",
    "    # Tokenize with return_offsets_mapping=True\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    \n",
    "    token_positions = {}\n",
    "    \n",
    "    for search_string in search_strings:\n",
    "        # Find character positions\n",
    "        start_char = text.find(search_string)\n",
    "        if start_char == -1:\n",
    "            continue\n",
    "        end_char = start_char + len(search_string)\n",
    "        \n",
    "        # Find corresponding token positions using offsets\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "        \n",
    "        for i, (token_start, token_end) in enumerate(encoding['offset_mapping'][0]):\n",
    "            # Find start token\n",
    "            if start_token is None and token_start <= start_char < token_end:\n",
    "                start_token = i\n",
    "            # Find end token\n",
    "            if end_token is None and token_start < end_char <= token_end:\n",
    "                end_token = i\n",
    "                break\n",
    "            # Handle case where end_char is exactly at token boundary\n",
    "            if end_token is None and token_start == end_char:\n",
    "                end_token = i - 1\n",
    "                break\n",
    "        \n",
    "        if start_token is not None and end_token is not None:\n",
    "            token_positions[search_string] = (start_token, end_token)\n",
    "    \n",
    "    return encoding, token_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_2 = \"Just vibe it.\"\n",
    "input_text_1 = (\n",
    "    \"A robust approach is to use trigonometry to solve the problem. Since the user has provided the angles and the length of the arm, \"\n",
    "    \"we can leverage trigonometric principles to construct a precise forward kinematics model. This method allows us to accurately compute the position of the arm's end effector, \"\n",
    "    \"generate the required code, and analyze the trajectory error, ensuring both correctness and efficiency in the solution.\"\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    f\"Idea 1: {input_text_1}\\n\"\n",
    "    f\"Idea 2: {input_text_2}\\n\"\n",
    "    \"You need to return your choice in json format as follows: {\\\"idea\\\": \\\"1\\\" or \\\"2\\\"}, nothing else. No additional text or comments.\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant helping to write code for a mechanical arm. You are presented with multiple ideas to solve the problem. You need to write the code for the best idea. You need to first select the idea 1 or 2 ONLY\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt},]\n",
    "        },\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_tokens = tokenizer.apply_chat_template(messages, tokenize=True, return_tensors=\"pt\", return_dict=True)\n",
    "# offsets      = tokenizer.apply_chat_template(messages, tokenize=False, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.bfloat16. This is not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using offset mappings:\n",
      "Idea 1: A robust approach is to use trigonometry to solve the problem. Since the user has provided the angles and the length of the arm, we can leverage trigonometric principles to construct a precise forward kinematics model. This method allows us to accurately compute the position of the arm's end effector, generate the required code, and analyze the trajectory error, ensuring both correctness and efficiency in the solution., 5, 80\n",
      "Idea 2: Just vibe it., 86, 89\n",
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant helping to write code for a mechanical arm. You are presented with multiple ideas to solve the problem. You need to write the code for the best idea. You need to first select the idea 1 or 2 ONLY\n",
      "\n",
      "Idea 1: A robust approach is to use trigonometry to solve the problem. Since the user has provided the angles and the length of the arm, we can leverage trigonometric principles to construct a precise forward kinematics model. This method allows us to accurately compute the position of the arm's end effector, generate the required code, and analyze the trajectory error, ensuring both correctness and efficiency in the solution.\n",
      "Idea 2: Just vibe it.\n",
      "You need to return your choice in json format as follows: {\"idea\": \"1\" or \"2\"}, nothing else. No additional text or comments.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "```json\n",
      "{\"idea\": \"1\"}\n",
      "```\n",
      "<end_of_turn>\n",
      "\n",
      " 5 80 <class 'int'> <class 'int'>\n",
      "Idea 1 mean: tensor([0.0008])\n",
      "\n",
      " 86 89 <class 'int'> <class 'int'>\n",
      "Idea 2 mean: tensor([0.0010])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test with our strings\n",
    "test_text = prompt\n",
    "search_strings = [input_text_1, input_text_2]\n",
    "\n",
    "\n",
    "# encoding, positions = find_token_positions_with_offsets(tokenizer, test_text, search_strings)\n",
    "\n",
    "print(\"Using offset mappings:\")\n",
    "idea_start_end = {}\n",
    "# for i, (text, (start, end)) in enumerate(positions.items()):\n",
    "#     print(f\"Idea {i+1}: {text}, {start}, {end}\")\n",
    "#     idea_start_end[i] = (start, end)\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device).to(torch.bfloat16)\n",
    "offsets = tokenizer.apply_chat_template(messages, tokenize=False, return_offsets_mapping=True)\n",
    "encoding, positions = find_token_positions_with_offsets(tokenizer, test_text, search_strings)\n",
    "\n",
    "for i, (text, (start, end)) in enumerate(positions.items()):\n",
    "    print(f\"Idea {i+1}: {text}, {start}, {end}\")\n",
    "    idea_start_end[i] = (start, end)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100, output_attentions=True, output_hidden_states=True, return_dict_in_generate=True)\n",
    "print(tokenizer.decode(outputs.sequences[0]))\n",
    "# The first level is the tokens level outputs.\n",
    "# The second level is the attention layers levels.\n",
    "# I need a tensor of the shape [batch_size, num_heads, seq_len + output_len, seq_len]\n",
    "layers_outs = torch.concat([layer[0] for layer in outputs.attentions[1:]], dim = -2)\n",
    "for idea_num, (start, end) in idea_start_end.items():\n",
    "    print('\\n', start, end, type(start), type(end))\n",
    "    idea_attention = layers_outs[:,:,:,start:end+1]\n",
    "    idea_mean = idea_attention.mean(dim=(1, 2, 3))  # (batch, 1, 1, 1)\n",
    "    print(f\"Idea {idea_num+1} mean: {idea_mean}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
